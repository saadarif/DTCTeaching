---
title: "Introduction to Statistical Data Analysis <br/> <br/>"
author: Saad Arif <br/>  Dept. of Biological and Medical Sciences & Centre for Functional
  Genomics <br/> Oxford Brookes University <br/>
date: "<small>Nov 2019</small>"
output:
  slidy_presentation:
    highlighter: prettify
    hitheme: tomorrow
    incremental: yes
    widescreen: yes
    mode: standalone
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align="center")
```


## Course Organization 


## Four Aspects of a Statistical Analysis <br /> <br /> 


1. Experimental Design
2. Exploratory Data Analysis (EDA)
3. Inference
4. Visualisation

We will discusss these steps of statistical analysis in order. But we might take some detours along the way where an important concept needs to be highlighted. 

## Experimental Design {.build}

- This is the part of statistics that happens before your experiment.

- The main aim of experimental design is to help establish a causal connection 
between the **independent** and **dependent** variable(s).

- Usually we measure something and then take an average (or some other *statistic*),
  
    estimated value = expected/real value + error + bias

- A major goal of experimental design is to:
      + avoid  bias
      + minimize sampling error
  
- **Independent variable**: These are the **factors**/variables you control in your experiment. Sometimes also known as the predictor variables. **Dependent variable**: This is the response variable you are interested in.


## What is Bias?

- Most of the time will be working with a *sample* from a larger *population* (we can't measure the entire population for various reasons)

- We need a representative/*random* sample from the population to make valid inferences (almost all statistical tests you will ever use implicitly assume random sampling)

- **Sources of Bias**: convenience sampling, voluntary response, lack of blinding

## Avoiding Bias

- Use of control groups

- Randomization 

- Blinding

## Sampling and Sampling Error {.build}

- We almost always sample from a population (randomly I hope)

- Differences between sample values (e.g. $\bar{X}$, *s* ) and population values ($\mu$, $\sigma$) are the sampling error

-  As long as we sample from a population some sampling error is unavoidable, but we can minimize it <center> ![](images/Outline Application For Nigel Groome Studentship Competition_SA.png) </center>

- **Blocking** is another way we minimize sampling error. This only works if we know some other variable (the block variable) adds uninteresting variability to our measurements

## Sampling and Sampling Distribution: Some Definitions {.build}

- $\bar{X}$ = sample mean,  $\mu$ = population mean (usually unknown), $\mu_{\bar{X}}$ = mean of a sample of $\bar{X}$'s

- s = sample standard deviation ,  $\sigma$ = population standard deviation (usually unknown)

- [sampling distributions in action](http://shiny.stat.calpoly.edu/Sampling_Distribution/)

## Central Limit Theorem (CLT) and Sampling Distributions

The CLT says that if you take many repeated samples from a population, and calculate the averages or sum of each one, the collection of those averages will be normally distributed...and it doesn’t matter what the shape of the source distribution is!

If you are measuring something and taking the average, then the average will be normally distributed or nearly normally distributed

## Central Limit Theorem (CLT) and Sampling Distributions: Die example

- What is the underlying probability distrbution of a fair six sided die?
  
    - a uniform distribution with six equal possibilites

- What is the expected value or average of rolling a die many time?
  
    - 3.5
  
- What happens if we throw a single die 10000 times?
```{r, fig.height=4, echo =F}
DieOutcome <- sample(1:6,10000, replace= TRUE)
hist(DieOutcome, col ="light blue")
abline(v=3.5, col = "red",lty=1)
```

## Central Limit Theorem (CLT) and Sampling Distributions: Die example

- Lets take a sample size of n=10 rolls from the previous simulation experiment (total of 10000 rolls of a single die) and take the mean of those 10 samples, repeat this 10000 times and plot a histogram of the means
```{r, echo =F}
x10 <- c()
k =10000
 for ( i in 1:k) {
 x10[i] = mean(sample(1:6,10, replace = TRUE))}
 hist(x10, col ="pink", main="Sample size =10",xlab ="Outcome of die roll")
 abline(v = mean(x10), col = "Red")
 abline(v = 3.5, col = "blue")
```

## Central Limit Theorem (CLT) and Sampling Distributions: Die example

-  What if I increase my sample size?
```{r, echo =F, fig.align="center"}
 x30 <- c()
 x100 <- c()
 x1000 <- c()
 k =10000
 for ( i in 1:k){
 x30[i] = mean(sample(1:6,30, replace = TRUE))
 x100[i] = mean(sample(1:6,100, replace = TRUE))
 x1000[i] = mean(sample(1:6,1000, replace = TRUE))
 }
 par(mfrow=c(1,3))
 hist(x30, col ="green",main="n=30",xlab ="die roll")
 abline(v = mean(x30), col = "blue")

 hist(x100, col ="light blue", main="n=100",xlab ="die roll")
 abline(v = mean(x100), col = "red")

 hist(x1000, col ="orange",main="n=1000",xlab ="die roll")
 abline(v = mean(x1000), col = "red")
```

## Central Limit Theorem (CLT) and Sampling Distributions

- Because of the CLT the normal distribution is (almost) everywhere and forms the basis of the many statistical tests

- The CLT states with increasing sample sizes (n > 30): $\mu_{\bar{X}}$ = $\mu$   and,  $\sigma_{\bar{X}}$ = $\frac{\sigma}{\sqrt{n}}$
   
-  $\sigma_{\bar{X}}$ is the standard deviation of the sampling distribution of the sample means, it also referred to as the standard error

- $\sigma_{\bar{X}}$ reflects the uncertainty in our **estimate of the mean**, we can use the $\sigma_{\bar{X}}$ and what we know about the normal distribution to calculate confidence intervals around our estimate

- in most cases  $\sigma$ will be unknown! but we have *s* an estimate of the standard deviation (more on this later)

- with increasing sample sizes $\bar{X}$ will converge to $\mu$ (*law of large numbers*)

## Exploratory Data Analysis (or EDA)

- You've got some data, what should you do next?

- EDA is an approach to data analysis that uses (mostly) visual tools to summarize the main characteristics of your dataset

- You may already have some formal analysis task (e.g. hypothesis testing) that you wish to conduct with your data set but EDA is still **very useful**

- Why EDA?

    - Transform or manipulate data so it is amenable for analysis, identify mistakes (data wrangling) <small>(we won't talk about this today but you will learn this later in the week) </small>
    - Suggest hypotheses about the causes of observed phenomena (data driven hypothesis)
    - Assess assumptions on which statistical inference will be based (tests for normality etc.)
    - Identify relationships among explanatory variables

- further reading: Tukey, John Wilder (1977). Exploratory Data Analysis. Addison-Wesley. ISBN 978-0-201-07616-5.

## Graphical Tools of EDA

**barplots** for categorical data

**histogram, dot plots, stem and leaf plots** to see the shape of numerical distributions

**boxplots** to see summaries of a numerical distribution, useful in comparing distributions and identifying long and short-tailed distributions.

**normal probability plots** To see if data is approximately normal
  
**PCA, multidimensional scaling, clustering techniques** multivariate methods for dimensionality reduction

Plus many more.... 

## Boxplots for quick summaries of data
```{r, echo=F, fig.align="center"}
x <- rnorm(100, mean=-2, sd=1)
x <- c(x, rnorm(100, mean=2, sd=1))
#hist(x)
boxplot(x,  ylab="some measurement")
```

Box = interquartile range 25 to 75  percentile (dispersion of the middle 50% of the values)
Whisker= range (max-min)
Bar = **median**

## Boxplots don't always tell the whole story
```{r, echo=F, fig.align="center"}
x <- rnorm(100, mean=-2, sd=1)
x <- c(x, rnorm(100, mean=2, sd=1))
par(mfrow=c(1,2))
hist(x, xlab="some measurement")
boxplot(x,  ylab="some measurement")
```

## Violin or Beeswarm plots are a better alternative (show all your data)
```{r, fig.align="center"}
if (!require(grid, quietly=TRUE)) {
  install.packages("grid")
  library(grid)
}
x <- rnorm(100, mean=-2, sd=1)
x <- c(x, rnorm(100, mean=2, sd=1))
par(mfrow=c(1,2))
#beeswarm boxplot
if (!require(beeswarm, quietly=TRUE)) {
  install.packages("beeswarm")
  library(beeswarm)
}
boxplot(x, ylab="some measurement")
beeswarm(x, add=TRUE)

#plotting ggplot2 and base plot in same row
vp.Right <- viewport(height=unit(.6, "npc"), width=unit(0.5, "npc"), 
                           just=c("left","top"), 
                           y=0.75, x=0.5)
# Violin plot
if (!require(ggplot2, quietly=TRUE)) {
  install.packages("ggplot2")
  library(ggplot2)
}
X <- as.data.frame(x)

p <- ggplot(X, aes(1,x)) + ylab("some measurement") + xlab("")
p <-p + geom_violin()
print(p, vp=vp.Right)

```

## Histograms for looking at distributions of data

We will work with the "iris" data set on the following slides. This dataset included measurement on 4 aspects of flower morphology for 3 different species of iris.
```{r iris, echo=F}
head(iris)
```

We might want to see if petal length is signficantly different among the species? We might want to see if the data are normally distributed.

```{r  echo=F, fig.height=6, fig.align="center"}
par(mfrow=c(1,3))
hist(iris$Petal.Length[iris$Species=="setosa"], main="Petal Length for I. setosa")
hist(iris$Petal.Length[iris$Species=="versicolor"], main="Petal Length for I. versicolor")
hist(iris$Petal.Length[iris$Species=="virginica"], main="Petal Length for I. virginica")
```
## QQ plots for assessing normality

Another graphical method for assessing normality. A Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line that’s roughly straight. 

What are **quantiles**? These are often referred to as “percentiles”. These are points in your data below which a certain proportion of your data fall. For example, imagine the classic bell-curve standard Normal distribution with a mean of 0. The 0.5 quantile, or 50th percentile, is 0. Half the data lie below 0. That’s the peak of the hump in the curve. The 0.95 quantile, or 95th percentile, is about 1.64. 95 percent of the data lie below 1.64.

Q-Q plots take your sample data, sort it in ascending order, and then plot them versus quantiles calculated from a theoretical distribution. The number of quantiles is selected to match the size of your sample data. While Normal Q-Q Plots are the ones most often used in practice due to so many statistical methods assuming normality, Q-Q Plots can actually be created for any distribution.

## QQ plots for assessing normality for petal length for three iris species

```{r  echo=F, fig.height=4, fig.width=9,fig.align="center"}
par(mfrow=c(1,3))
qqnorm(iris$Petal.Length[iris$Species=="setosa"], main="Petal Length for I. setosa")
qqline(iris$Petal.Length[iris$Species=="setosa"])
qqnorm(iris$Petal.Length[iris$Species=="versicolor"], main="Petal Length for I. versicolor")
qqline(iris$Petal.Length[iris$Species=="versicolor"])
qqnorm(iris$Petal.Length[iris$Species=="virginica"], main="Petal Length for I. virginica")
qqline(iris$Petal.Length[iris$Species=="virginica"])
```

1. light tails         
2. skew to the left      
3. skew to the right

## Multiple boxplots for quick patterns

```{r, fig.align="center"}
par(mfrow=c(2,2))
boxplot(iris$Sepal.Length ~ iris$Species, main="Sepal Length")
boxplot(iris$Sepal.Width ~ iris$Species, main="Sepal Width")
boxplot(iris$Petal.Length ~ iris$Species, main="Petal Length")
boxplot(iris$Petal.Width ~ iris$Species, main="Petal Length")
```

## Scatterplots for patterns and evaluating relationships between variables

```{r , fig.height=7, fig.width=7, fig.align="center"}
pairs(iris[1:4], main = "Iris data red:setosa, green:versicolor, blue:virginca", pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)], lower.panel=NULL, labels=c("SL","SW","PL","PW"), font.labels=2, cex.labels=4.5) 
```

## Princpal Component Analysis for Patterns 

Principal Components is sometimes referred to as a **dimensionality reduction** technique. It tries to find the orthogonal (uncorrelated) axes of variation in your dataset and then rotates your data set to align with maximal axes of variation (in descending order) 

```{r, warning=F, message=F, fig.align="center"}
library(dplyr)
library(grid)
library(cowplot) #too arrange plots in a grid
iris %>% select(-Species) %>% # remove Species column
  scale() %>%                 # scale to 0 mean and unit variance
  prcomp() ->                 # do PCA
  pca                         # store result as `pca`

pca_data <- data.frame(pca$x, Species=iris$Species)
p1<- ggplot(pca_data, aes(x=PC1, y=PC2, color=Species)) + geom_point()

# capture the rotation matrix in a data frame
rotation_data <- data.frame(pca$rotation, variable=row.names(pca$rotation))
# define a pleasing arrow style
arrow_style <- arrow(length = unit(0.05, "inches"),
                     type = "closed")
# now plot, using geom_segment() for arrows and geom_text for labels
p2 <- ggplot(rotation_data) + 
  geom_segment(aes(xend=PC1, yend=PC2), x=0, y=0, arrow=arrow_style) + 
  geom_text(aes(x=PC1, y=PC2, label=variable), hjust=0, size=3, color='red') + 
  xlim(-1.,1.25) + 
  ylim(-1.,1.) +
  coord_fixed() # fix aspect ratio to 1:1

percent <- 100*pca$sdev^2/sum(pca$sdev^2)
perc_data <- data.frame(percent=percent, PC=1:length(percent))
p3 <- ggplot(perc_data, aes(x=PC, y=percent)) + 
  geom_bar(stat="identity") + 
  geom_text(aes(label=round(percent, 2)), size=4, vjust=-.5) + 
  ylim(0, 80)

plot_grid(p1, p2, p3, labels = "AUTO")
```

## Princpal Component Analysis for batch correction 

PCA can be used to get of systematic bias in data collection (over time, different machines etc.)

<center>![](images/PCAbatchcorrection_Fig2.gig.gif) </center>



<font size="-1">from: Zhu, X., Wolfgruber, T.K., Tasato, A. et al. Genome Med (2017) 9: 108. https://doi.org/10.1186/s13073-017-0492-3 </font>

## Princpal Component Analysis for general patterns in mutlivariate data

PCA is a **general pattern engine** that is useful to look for patterns in your data without any prior knowledge of groupings or clusters (in machine learning terminology is is unsupervised clustering)

<center><img src="images/nihms132060f1.jpg"/></center>


<font size="-1"> from: Novembre J, Johnson T, Bryc K, et al. Genes mirror geography within Europe [published correction appears in Nature. 2008 Nov 13;456(7219):274]. Nature. 2008;456(7218):98–101. doi:10.1038/nature07331 </font>


## Other Clustering Techniques

```{r, echo=FALSE,out.width="49%",out.height="20%",fig.cap=" ",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/heirarchical.png","images/entire_clust.svg"))
``` 

<font size="-1">from :http://gastrulation.stemcells.cam.ac.uk, 
Mishra, A.K., Duraisamy, G.S., Khare, M. et al. Genome-wide transcriptome profiling of transgenic hop (Humulus lupulus L.) constitutively overexpressing HlWRKY1 and HlWDR1 transcription factors. BMC Genomics 19, 739 (2018) doi:10.1186/s12864-018-5125-8 </font>


## Summary for EDA {.build}

> - Exploratory plots are "quick and dirty" (*these are not meant as final figures for your publications*)

> - Let you summarize the data (usually graphically) and highlight any broad features

> - Explore basic questions and hypotheses (and perhaps rule them out)

> - Suggest modeling strategies and check assumptions for the "next step"

> - Can identify additional patterns in the data

# Statistical Inference

## Motivation for Statistical Inference

## Frequentist vs. Bayesian Statisitics

## Machine Learning vs. Statistical Inference

## Confidence Intervals??

## Data Types

## Hypothesis Testing

## Hypothesis Types

## Test types

## Type 1 type 2 errors

## p-values

## p-values vs. effect size

## multiple testing Problem

## FWER

## FDR


